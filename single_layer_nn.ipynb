{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of the single-layer neural network, including training, testing, and evaluation on the Iris dataset."
      ],
      "metadata": {
        "id": "yAANJrxvalf4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmapMdvhMzEV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleLayerNN:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.W = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.b = np.zeros((1, output_size))\n",
        "\n",
        "    def affine_forward(self, X):\n",
        "        self.X = X\n",
        "        return np.dot(X, self.W) + self.b\n",
        "\n",
        "    def affine_backward(self, dloss):\n",
        "        dW = np.dot(self.X.T, dloss)\n",
        "        db = np.sum(dloss, axis=0)\n",
        "        dX = np.dot(dloss, self.W.T)\n",
        "        return dW, db, dX\n",
        "\n",
        "    def SVM(self, logits, Y):\n",
        "        diff = logits - logits[np.arange(Y.shape[0]), Y][:, np.newaxis]\n",
        "        diff[diff < 0] = 0\n",
        "        loss = np.sum(diff)\n",
        "        diff[diff > 0] = 1\n",
        "        row_sum = np.sum(diff, axis=1)\n",
        "        row_sum = -1 + row_sum\n",
        "        diff[np.arange(Y.shape[0]), Y] = row_sum\n",
        "        return loss, diff\n",
        "\n",
        "    def loss(self, logits, Y):\n",
        "        loss, _ = self.SVM(logits, Y)\n",
        "        return loss\n",
        "\n",
        "    def train(self, X, Y, learning_rate=0.00000001, num_iters=1000):\n",
        "        for i in range(num_iters):\n",
        "            logits = self.affine_forward(X)\n",
        "            loss, dloss = self.SVM(logits, Y)\n",
        "            dW, db, _ = self.affine_backward(dloss)\n",
        "            self.W -= learning_rate * dW\n",
        "            self.b -= learning_rate * db\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Iteration {i}, Loss: {loss}\")\n",
        "\n",
        "    def test(self, X_test, Y_test):\n",
        "        logits = self.affine_forward(X_test)\n",
        "        Y_pred = np.argmax(logits, axis=1)  # Predict class labels\n",
        "        accuracy = np.mean(Y_pred == Y_test)\n",
        "        print(\"Test accuracy:\", accuracy)\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "Y = iris.target\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = SingleLayerNN(X.shape[1], len(np.unique(Y)))\n",
        "model.train(X, Y)\n",
        "\n",
        "test_accuracy = model.test(X_test, Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Piq5926JtK3j",
        "outputId": "b04d1374-39ba-498c-d434-9bdaeda6ce72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 2.279368545297343\n",
            "Iteration 10, Loss: 2.2937020591808617\n",
            "Iteration 20, Loss: 2.3097701026176765\n",
            "Iteration 30, Loss: 2.3268879775780213\n",
            "Iteration 40, Loss: 2.344502950578021\n",
            "Iteration 50, Loss: 2.3621179235780208\n",
            "Iteration 60, Loss: 2.3797328965780205\n",
            "Iteration 70, Loss: 2.398767192012701\n",
            "Iteration 80, Loss: 2.418152722012702\n",
            "Iteration 90, Loss: 2.437538252012703\n",
            "Iteration 100, Loss: 2.4577667378351498\n",
            "Iteration 110, Loss: 2.478192109835142\n",
            "Iteration 120, Loss: 2.4990780357457814\n",
            "Iteration 130, Loss: 2.520070185745782\n",
            "Iteration 140, Loss: 2.5416602759969615\n",
            "Iteration 150, Loss: 2.5633606399969593\n",
            "Iteration 160, Loss: 2.5858212415175315\n",
            "Iteration 170, Loss: 2.6086075125175303\n",
            "Iteration 180, Loss: 2.6322140126303557\n",
            "Iteration 190, Loss: 2.6559807715498414\n",
            "Iteration 200, Loss: 2.680291624549841\n",
            "Iteration 210, Loss: 2.7053632004417065\n",
            "Iteration 220, Loss: 2.730614055441711\n",
            "Iteration 230, Loss: 2.7558649104417148\n",
            "Iteration 240, Loss: 2.7812156566003794\n",
            "Iteration 250, Loss: 2.807326311600381\n",
            "Iteration 260, Loss: 2.8334369666003827\n",
            "Iteration 270, Loss: 2.8595476216003846\n",
            "Iteration 280, Loss: 2.8856582766003864\n",
            "Iteration 290, Loss: 2.9117689316003883\n",
            "Iteration 300, Loss: 2.9380148808376823\n",
            "Iteration 310, Loss: 2.9647874398376852\n",
            "Iteration 320, Loss: 2.9915599988376886\n",
            "Iteration 330, Loss: 3.018332557837692\n",
            "Iteration 340, Loss: 3.0451051168376955\n",
            "Iteration 350, Loss: 3.0718776758376984\n",
            "Iteration 360, Loss: 3.0986502348377023\n",
            "Iteration 370, Loss: 3.1254227938377053\n",
            "Iteration 380, Loss: 3.152374281996118\n",
            "Iteration 390, Loss: 3.180595874837289\n",
            "Iteration 400, Loss: 3.2091915088372955\n",
            "Iteration 410, Loss: 3.237880834223012\n",
            "Iteration 420, Loss: 3.267317274223013\n",
            "Iteration 430, Loss: 3.2967537142230148\n",
            "Iteration 440, Loss: 3.326190154223016\n",
            "Iteration 450, Loss: 3.3556265942230175\n",
            "Iteration 460, Loss: 3.385063034223019\n",
            "Iteration 470, Loss: 3.41449947422302\n",
            "Iteration 480, Loss: 3.443935914223021\n",
            "Iteration 490, Loss: 3.473372354223023\n",
            "Iteration 500, Loss: 3.502808794223024\n",
            "Iteration 510, Loss: 3.532245234223025\n",
            "Iteration 520, Loss: 3.5616816742230264\n",
            "Iteration 530, Loss: 3.591118114223029\n",
            "Iteration 540, Loss: 3.6205545542230295\n",
            "Iteration 550, Loss: 3.649990994223031\n",
            "Iteration 560, Loss: 3.679427434223032\n",
            "Iteration 570, Loss: 3.708863874223034\n",
            "Iteration 580, Loss: 3.738300314223035\n",
            "Iteration 590, Loss: 3.767736754223036\n",
            "Iteration 600, Loss: 3.797173194223038\n",
            "Iteration 610, Loss: 3.82660963422304\n",
            "Iteration 620, Loss: 3.856046074223041\n",
            "Iteration 630, Loss: 3.8854825142230425\n",
            "Iteration 640, Loss: 3.9150266211563745\n",
            "Iteration 650, Loss: 3.9453885801563726\n",
            "Iteration 660, Loss: 3.9757505391563694\n",
            "Iteration 670, Loss: 4.006112498156368\n",
            "Iteration 680, Loss: 4.036474457156365\n",
            "Iteration 690, Loss: 4.066836416156363\n",
            "Iteration 700, Loss: 4.097198375156361\n",
            "Iteration 710, Loss: 4.127560334156358\n",
            "Iteration 720, Loss: 4.157922293156356\n",
            "Iteration 730, Loss: 4.188284252156355\n",
            "Iteration 740, Loss: 4.2186462111563525\n",
            "Iteration 750, Loss: 4.249008170156351\n",
            "Iteration 760, Loss: 4.27937012915635\n",
            "Iteration 770, Loss: 4.309732088156347\n",
            "Iteration 780, Loss: 4.340094047156345\n",
            "Iteration 790, Loss: 4.370456006156344\n",
            "Iteration 800, Loss: 4.400817965156342\n",
            "Iteration 810, Loss: 4.43117992415634\n",
            "Iteration 820, Loss: 4.46154188315634\n",
            "Iteration 830, Loss: 4.491903842156336\n",
            "Iteration 840, Loss: 4.522265801156335\n",
            "Iteration 850, Loss: 4.552627760156334\n",
            "Iteration 860, Loss: 4.5829897191563305\n",
            "Iteration 870, Loss: 4.61335167815633\n",
            "Iteration 880, Loss: 4.643713637156328\n",
            "Iteration 890, Loss: 4.674075596156326\n",
            "Iteration 900, Loss: 4.704437555156325\n",
            "Iteration 910, Loss: 4.734799514156322\n",
            "Iteration 920, Loss: 4.76516147315632\n",
            "Iteration 930, Loss: 4.79552343215632\n",
            "Iteration 940, Loss: 4.8258853911563175\n",
            "Iteration 950, Loss: 4.856247350156316\n",
            "Iteration 960, Loss: 4.886609309156315\n",
            "Iteration 970, Loss: 4.9169712681563125\n",
            "Iteration 980, Loss: 4.94733322715631\n",
            "Iteration 990, Loss: 4.97769518615631\n",
            "Test accuracy: 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class SingleLayerNN:\n",
        "#     def __init__(self, input_size, output_size):\n",
        "#         self.W = np.random.randn(input_size, output_size) * 0.01\n",
        "#         self.b = np.zeros((1, output_size))\n",
        "\n",
        "#     def affine_forward(self, X):\n",
        "#         return np.dot(X, self.W) + self.b\n",
        "\n",
        "#     def SVM(self, logits, Y):\n",
        "#         diff = logits - logits[np.arange(Y.shape[0]), Y][:, np.newaxis]\n",
        "#         diff[diff < 0] = 0\n",
        "#         loss = np.sum(diff)\n",
        "#         diff[diff > 0] = 1\n",
        "#         row_sum = np.sum(diff, axis=1)\n",
        "#         row_sum = -1 + row_sum\n",
        "#         diff[np.arange(Y.shape[0]), Y] = row_sum\n",
        "#         return loss, diff\n",
        "\n",
        "#     def train(self, X, Y, learning_rate=0.0000000001, num_iters=10000):\n",
        "#         for i in range(num_iters):\n",
        "#             logits = self.affine_forward(X)\n",
        "#             loss, dloss = self.SVM(logits, Y)\n",
        "#             self.W -= learning_rate * np.dot(X.T, dloss)\n",
        "#             self.b -= learning_rate * np.sum(dloss, axis=0)\n",
        "#             if i % 1000 == 0:\n",
        "#                 print(f\"Iteration {i}, Loss: {loss}\")\n",
        "\n",
        "#     def test(self, X_test, Y_test):\n",
        "#         logits = self.affine_forward(X_test)\n",
        "#         Y_pred = np.argmax(logits, axis=1)  # Predict class labels\n",
        "#         accuracy = np.mean(Y_pred == Y_test)\n",
        "#         print(\"Test accuracy:\", accuracy)\n",
        "#         return accuracy\n",
        "\n",
        "\n",
        "# iris = datasets.load_iris()\n",
        "# X = iris.data\n",
        "# Y = iris.target\n",
        "\n",
        "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# model = SingleLayerNN(X.shape[1], len(np.unique(Y)))\n",
        "# model.train(X, Y)\n",
        "\n",
        "# test_accuracy = model.test(X_test, Y_test)"
      ],
      "metadata": {
        "id": "Smk3dzODTzyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F2HmTlmJBXFS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}